{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra imports for tablebench example\n",
    "import rtdl\n",
    "from tablebench.core import TabularDataset, TabularDatasetConfig\n",
    "\n",
    "from tablebench.datasets.experiment_configs import EXPERIMENT_CONFIGS\n",
    "from tablebench.models import get_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"adult\"\n",
    "expt_config = EXPERIMENT_CONFIGS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = TabularDatasetConfig()\n",
    "dset = TabularDataset(experiment,\n",
    "                      config=dataset_config,\n",
    "                      splitter=expt_config.splitter,\n",
    "                      grouper=expt_config.grouper,\n",
    "                      preprocessor_config=expt_config.preprocessor_config,\n",
    "                      **expt_config.tabular_dataset_kwargs)\n",
    "train_loader = dset.get_dataloader(\"train\", 512)\n",
    "loaders = {s: dset.get_dataloader(s, 2048) for s in (\"validation\", \"test\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adult(config):\n",
    "    loss_fn = F.binary_cross_entropy_with_logits\n",
    "    \n",
    "    model = get_estimator(\"mlp\", d_in=dset.X_shape[1], d_layers=[config[\"d_hidden\"]] * config[\"num_layers\"])\n",
    "    optimizer = (\n",
    "        model.make_default_optimizer()\n",
    "        if isinstance(model, rtdl.FTTransformer)\n",
    "        else torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"]))\n",
    "    \n",
    "    # Fit the model; results on validation split are reported to tune.\n",
    "    model.fit(train_loader, optimizer, loss_fn, n_epochs=5, other_loaders=loaders, tune_report_split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    # Sample a float uniformly between 0.0001 and 0.1, while\n",
    "    # sampling in log space and rounding to multiples of 0.00005\n",
    "    \"lr\": tune.qloguniform(1e-4, 1e-1, 5e-5),\n",
    "    \n",
    "    # Sample a float uniformly between 0 and 1,\n",
    "    # rounding to multiples of 0.1\n",
    "    \"weight_decay\": tune.quniform(0., 1., 0.1),\n",
    "    \n",
    "    # Random integer between 1 and 4\n",
    "    \"num_layers\": tune.randint(1,4),\n",
    "    \n",
    "    # Random integer from set\n",
    "    \"d_hidden\": tune.choice([64, 128, 256, 512])\n",
    "}\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_adult,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.tune_config.TuneConfig(num_samples=5),\n",
    "    run_config=air.RunConfig(local_dir=\"./results\", name=\"test_experiment\"),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0].log_dir)\n",
    "results[0].metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {result.log_dir: result.metrics_dataframe for result in results}\n",
    "[d._metric.plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfs.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray docs example\n",
    "\n",
    "via https://docs.ray.io/en/latest/ray-air/examples/torch_image_example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 22:36:57,514\tINFO worker.py:1525 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2022-11-24 22:37:01,846\tWARNING read_api.py:297 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "\u001b[2m\u001b[36m(_get_read_tasks pid=7758)\u001b[0m 2022-11-24 22:37:01,843\tWARNING torch_datasource.py:56 -- `SimpleTorchDatasource` doesn't support parallel reads. The `parallelism` argument will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_execute_read_task_nosplit pid=7758)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_get_read_tasks pid=7758)\u001b[0m 2022-11-24 22:37:22,858\tWARNING torch_datasource.py:56 -- `SimpleTorchDatasource` doesn't support parallel reads. The `parallelism` argument will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_execute_read_task_nosplit pid=7758)\u001b[0m Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.data.datasource import SimpleTorchDatasource\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "def train_dataset_factory():\n",
    "    return torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", download=True, train=True, transform=transform\n",
    "    )\n",
    "\n",
    "\n",
    "def test_dataset_factory():\n",
    "    return torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", download=True, train=False, transform=transform\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset: ray.data.Dataset = ray.data.read_datasource(\n",
    "    SimpleTorchDatasource(), dataset_factory=train_dataset_factory\n",
    ")\n",
    "test_dataset: ray.data.Dataset = ray.data.read_datasource(\n",
    "    SimpleTorchDatasource(), dataset_factory=test_dataset_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 22:37:27,704\tWARNING dataset.py:3818 -- 'ipywidgets' isn't installed. Run `pip install ipywidgets` to enable notebook widgets.\n"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    model = train.torch.prepare_model(Net())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_dataset_shard = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    for epoch in range(2):\n",
    "        running_loss = 0.0\n",
    "        train_dataset_batches = train_dataset_shard.iter_torch_batches(\n",
    "            batch_size=config[\"batch_size\"],\n",
    "        )\n",
    "        for i, batch in enumerate(train_dataset_batches):\n",
    "            # get the inputs and labels\n",
    "            inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        metrics = dict(running_loss=running_loss)\n",
    "        checkpoint = TorchCheckpoint.from_state_dict(model.module.state_dict())\n",
    "        session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-24 22:38:07</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:39.30        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.2/8.0 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/2.3 GiB heap, 0.0/1.15 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7c272_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/jpgard/ray_results/TorchTrainer_2022-11-24_22-37-28/TorchTrainer_7c272_00000_0_2022-11-24_22-37-28/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7c272_00000</td><td>ERROR   </td><td>127.0.0.1:7771</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=7779)\u001b[0m 2022-11-24 22:37:38,538\tINFO config.py:88 -- Setting up process group for: env:// [rank=0, world_size=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=7789)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:07,170 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 12514869248; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "2022-11-24 22:38:07,661\tERROR trial_runner.py:993 -- Trial TorchTrainer_7c272_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/_private/worker.py\", line 2289, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError: \u001b[36mray::_Inner.train()\u001b[39m (pid=7771, ip=127.0.0.1, repr=TorchTrainer)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 328, in entrypoint\n",
      "    self._status_reporter.get_checkpoint(),\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 475, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 651, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 390, in train_func\n",
      "    trainer.training_loop()\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/data_parallel_trainer.py\", line 368, in training_loop\n",
      "    checkpoint_strategy=None,\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 154, in __init__\n",
      "    checkpoint_strategy=checkpoint_strategy,\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 179, in _start_training\n",
      "    lambda: self._backend_executor.start_training(\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 188, in _run_with_error_handling\n",
      "    return func()\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 182, in <lambda>\n",
      "    checkpoint=checkpoint,\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/backend_executor.py\", line 332, in start_training\n",
      "    self.dataset_shards = dataset_spec.get_dataset_shards(actors)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/dataset_spec.py\", line 211, in get_dataset_shards\n",
      "    locality_hints=training_worker_handles,\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/dataset.py\", line 984, in split\n",
      "    blocks = self._plan.execute()\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 309, in execute\n",
      "    blocks, clear_input_blocks, self._run_by_consumer\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 672, in __call__\n",
      "    fn_constructor_kwargs=self.fn_constructor_kwargs,\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 128, in _apply\n",
      "    raise e from None\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 115, in _apply\n",
      "    results = map_bar.fetch_until_complete(refs)\n",
      "  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n",
      "    for ref, result in zip(done, ray.get(done)):\n",
      "ray.exceptions.RayTaskError: \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=7789, ip=127.0.0.1)\n",
      "ray.exceptions.OutOfDiskError: Local disk is full\n",
      "The object cannot be created because the local object store is full and the local disk's utilization is over capacity (95% by default).Tip: Use `df` on this node to check disk usage and `ray memory` to check object store memory usage.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>date               </th><th>experiment_id                   </th><th>hostname                    </th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7c272_00000</td><td>2022-11-24_22-37-33</td><td>390045c512d349feabb57aa39b055f79</td><td>Joshuas-MacBook-Pro-10.local</td><td>127.0.0.1</td><td style=\"text-align: right;\"> 7771</td><td style=\"text-align: right;\"> 1669347453</td><td>7c272_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 22:38:07,863\tERROR tune.py:773 -- Trials did not complete: [TorchTrainer_7c272_00000]\n",
      "2022-11-24 22:38:07,864\tINFO tune.py:778 -- Total run time: 39.53 seconds (39.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray::_Inner.train()\u001b[39m (pid=7771, ip=127.0.0.1, repr=TorchTrainer)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n    raise skipped from exception_cause(skipped)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 328, in entrypoint\n    self._status_reporter.get_checkpoint(),\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 475, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 651, in _trainable_func\n    output = fn()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 390, in train_func\n    trainer.training_loop()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/data_parallel_trainer.py\", line 368, in training_loop\n    checkpoint_strategy=None,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 154, in __init__\n    checkpoint_strategy=checkpoint_strategy,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 179, in _start_training\n    lambda: self._backend_executor.start_training(\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 188, in _run_with_error_handling\n    return func()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 182, in <lambda>\n    checkpoint=checkpoint,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/backend_executor.py\", line 332, in start_training\n    self.dataset_shards = dataset_spec.get_dataset_shards(actors)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/dataset_spec.py\", line 211, in get_dataset_shards\n    locality_hints=training_worker_handles,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/dataset.py\", line 984, in split\n    blocks = self._plan.execute()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 309, in execute\n    blocks, clear_input_blocks, self._run_by_consumer\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 672, in __call__\n    fn_constructor_kwargs=self.fn_constructor_kwargs,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 128, in _apply\n    raise e from None\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 115, in _apply\n    results = map_bar.fetch_until_complete(refs)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n    for ref, result in zip(done, ray.get(done)):\nray.exceptions.RayTaskError: \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=7789, ip=127.0.0.1)\nray.exceptions.OutOfDiskError: Local disk is full\nThe object cannot be created because the local object store is full and the local disk's utilization is over capacity (95% by default).Tip: Use `df` on this node to check disk usage and `ray memory` to check object store memory usage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3f/96kfmy7s1y5_5fr8cgqtfzpm0000gn/T/ipykernel_7730/4251098542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mscaling_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScalingConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlatest_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTuneError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTrainingFailedError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=7771, ip=127.0.0.1, repr=TorchTrainer)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 355, in train\n    raise skipped from exception_cause(skipped)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 328, in entrypoint\n    self._status_reporter.get_checkpoint(),\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 475, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/tune/trainable/function_trainable.py\", line 651, in _trainable_func\n    output = fn()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/base_trainer.py\", line 390, in train_func\n    trainer.training_loop()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/data_parallel_trainer.py\", line 368, in training_loop\n    checkpoint_strategy=None,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 154, in __init__\n    checkpoint_strategy=checkpoint_strategy,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 179, in _start_training\n    lambda: self._backend_executor.start_training(\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 188, in _run_with_error_handling\n    return func()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/trainer.py\", line 182, in <lambda>\n    checkpoint=checkpoint,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/backend_executor.py\", line 332, in start_training\n    self.dataset_shards = dataset_spec.get_dataset_shards(actors)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/train/_internal/dataset_spec.py\", line 211, in get_dataset_shards\n    locality_hints=training_worker_handles,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/dataset.py\", line 984, in split\n    blocks = self._plan.execute()\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 309, in execute\n    blocks, clear_input_blocks, self._run_by_consumer\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/plan.py\", line 672, in __call__\n    fn_constructor_kwargs=self.fn_constructor_kwargs,\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 128, in _apply\n    raise e from None\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/compute.py\", line 115, in _apply\n    results = map_bar.fetch_until_complete(refs)\n  File \"/Users/jpgard/Documents/github/tablebench/venv3.7/lib/python3.7/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n    for ref, result in zip(done, ray.get(done)):\nray.exceptions.RayTaskError: \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=7789, ip=127.0.0.1)\nray.exceptions.OutOfDiskError: Local disk is full\nThe object cannot be created because the local object store is full and the local disk's utilization is over capacity (95% by default).Tip: Use `df` on this node to check disk usage and `ray memory` to check object store memory usage."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:17,232 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11884355584; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:27,302 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11884359680; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:37,370 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11883663360; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:47,427 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11883708416; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:38:57,511 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11877466112; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:07,524 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11859861504; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:17,602 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11859570688; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:27,663 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11856936960; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:37,734 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11853127680; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:47,801 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11853119488; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:39:57,857 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11853127680; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:40:07,926 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11853033472; capacity: 250685575168. Object creation will fail if spilling is required.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-11-24 22:40:17,995 E 7748 352103] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-11-24_22-36-53_113079_7730 is over 95% full, available space: 11852312576; capacity: 250685575168. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"batch_size\": 2},\n",
    "    datasets={\"train\": train_dataset},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    ")\n",
    "result = trainer.fit()\n",
    "latest_checkpoint = result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tableshift3.7",
   "language": "python",
   "name": "tableshift3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
